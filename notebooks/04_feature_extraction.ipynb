{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Pipeline\n",
    "\n",
    "This notebook extracts various features from plant images for disease detection and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from skimage.feature import local_binary_pattern, greycomatrix, greycoprops\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import filters, exposure\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "PROJECT_ROOT = Path('/Users/debabratapattnayak/web-dev/greencast')\n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / 'processed_data'\n",
    "FEATURES_PATH = PROJECT_ROOT / 'features'\n",
    "FEATURES_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Computer Vision Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalFeatureExtractor:\n",
    "    \"\"\"Extract traditional computer vision features from images\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def extract_color_features(self, image):\n",
    "        \"\"\"Extract color-based features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # RGB statistics\n",
    "        for i, channel in enumerate(['R', 'G', 'B']):\n",
    "            channel_data = image[:, :, i]\n",
    "            features.extend([\n",
    "                np.mean(channel_data),\n",
    "                np.std(channel_data),\n",
    "                np.median(channel_data),\n",
    "                np.percentile(channel_data, 25),\n",
    "                np.percentile(channel_data, 75)\n",
    "            ])\n",
    "        \n",
    "        # HSV statistics\n",
    "        for i, channel in enumerate(['H', 'S', 'V']):\n",
    "            channel_data = hsv[:, :, i]\n",
    "            features.extend([\n",
    "                np.mean(channel_data),\n",
    "                np.std(channel_data)\n",
    "            ])\n",
    "        \n",
    "        # Color ratios\n",
    "        features.extend([\n",
    "            np.mean(image[:, :, 1]) / (np.mean(image[:, :, 0]) + 1e-8),  # G/R ratio\n",
    "            np.mean(image[:, :, 2]) / (np.mean(image[:, :, 1]) + 1e-8),  # B/G ratio\n",
    "            np.mean(image[:, :, 0]) / (np.mean(image[:, :, 2]) + 1e-8),  # R/B ratio\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_texture_features(self, image):\n",
    "        \"\"\"Extract texture-based features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Local Binary Pattern\n",
    "        radius = 3\n",
    "        n_points = 8 * radius\n",
    "        lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "        lbp_hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, \n",
    "                                  range=(0, n_points + 2), density=True)\n",
    "        features.extend(lbp_hist)\n",
    "        \n",
    "        # Gray Level Co-occurrence Matrix (GLCM)\n",
    "        # Normalize gray image to 0-255 range for GLCM\n",
    "        gray_norm = ((gray - gray.min()) / (gray.max() - gray.min()) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Calculate GLCM for different angles\n",
    "        distances = [1, 2]\n",
    "        angles = [0, 45, 90, 135]\n",
    "        \n",
    "        for distance in distances:\n",
    "            for angle in angles:\n",
    "                glcm = greycomatrix(gray_norm, [distance], [np.radians(angle)], \n",
    "                                  levels=256, symmetric=True, normed=True)\n",
    "                \n",
    "                # Extract GLCM properties\n",
    "                features.extend([\n",
    "                    greycoprops(glcm, 'contrast')[0, 0],\n",
    "                    greycoprops(glcm, 'dissimilarity')[0, 0],\n",
    "                    greycoprops(glcm, 'homogeneity')[0, 0],\n",
    "                    greycoprops(glcm, 'energy')[0, 0],\n",
    "                    greycoprops(glcm, 'correlation')[0, 0]\n",
    "                ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_shape_features(self, image):\n",
    "        \"\"\"Extract shape-based features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Convert to grayscale and threshold\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Apply Otsu's thresholding\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if contours:\n",
    "            # Get the largest contour (assuming it's the main object)\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Calculate shape features\n",
    "            area = cv2.contourArea(largest_contour)\n",
    "            perimeter = cv2.arcLength(largest_contour, True)\n",
    "            \n",
    "            if perimeter > 0:\n",
    "                # Circularity\n",
    "                circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "                features.append(circularity)\n",
    "            else:\n",
    "                features.append(0)\n",
    "            \n",
    "            # Aspect ratio\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            aspect_ratio = float(w) / h if h > 0 else 0\n",
    "            features.append(aspect_ratio)\n",
    "            \n",
    "            # Extent (ratio of contour area to bounding rectangle area)\n",
    "            rect_area = w * h\n",
    "            extent = float(area) / rect_area if rect_area > 0 else 0\n",
    "            features.append(extent)\n",
    "            \n",
    "            # Solidity (ratio of contour area to convex hull area)\n",
    "            hull = cv2.convexHull(largest_contour)\n",
    "            hull_area = cv2.contourArea(hull)\n",
    "            solidity = float(area) / hull_area if hull_area > 0 else 0\n",
    "            features.append(solidity)\n",
    "            \n",
    "        else:\n",
    "            # No contours found, add default values\n",
    "            features.extend([0, 0, 0, 0])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, image):\n",
    "        \"\"\"Extract all traditional features from an image\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Extract different types of features\n",
    "        color_features = self.extract_color_features(image)\n",
    "        texture_features = self.extract_texture_features(image)\n",
    "        shape_features = self.extract_shape_features(image)\n",
    "        \n",
    "        features.extend(color_features)\n",
    "        features.extend(texture_features)\n",
    "        features.extend(shape_features)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "# Initialize feature extractor\n",
    "traditional_extractor = TraditionalFeatureExtractor()\n",
    "print(\"Traditional feature extractor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFeatureExtractor:\n",
    "    \"\"\"Extract features using pre-trained deep learning models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load pre-trained models for feature extraction\"\"\"\n",
    "        print(\"Loading pre-trained models...\")\n",
    "        \n",
    "        # VGG16\n",
    "        self.models['vgg16'] = VGG16(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            pooling='avg',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        # ResNet50\n",
    "        self.models['resnet50'] = ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            pooling='avg',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        # EfficientNetB0\n",
    "        self.models['efficientnet'] = EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            pooling='avg',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        print(\"Models loaded successfully!\")\n",
    "    \n",
    "    def preprocess_image(self, img, model_name):\n",
    "        \"\"\"Preprocess image for specific model\"\"\"\n",
    "        # Ensure image is in correct format\n",
    "        if img.shape != (224, 224, 3):\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "        \n",
    "        # Expand dimensions for batch processing\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Apply model-specific preprocessing\n",
    "        if model_name == 'vgg16':\n",
    "            return vgg_preprocess(img.copy())\n",
    "        elif model_name == 'resnet50':\n",
    "            return resnet_preprocess(img.copy())\n",
    "        elif model_name == 'efficientnet':\n",
    "            return efficientnet_preprocess(img.copy())\n",
    "        else:\n",
    "            return img / 255.0\n",
    "    \n",
    "    def extract_features(self, image, model_name='vgg16'):\n",
    "        \"\"\"Extract features using specified model\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not available\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_img = self.preprocess_image(image, model_name)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.models[model_name].predict(processed_img, verbose=0)\n",
    "        \n",
    "        return features.flatten()\n",
    "    \n",
    "    def extract_all_model_features(self, image):\n",
    "        \"\"\"Extract features from all available models\"\"\"\n",
    "        all_features = {}\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            features = self.extract_features(image, model_name)\n",
    "            all_features[model_name] = features\n",
    "        \n",
    "        return all_features\n",
    "\n",
    "# Initialize deep feature extractor\n",
    "deep_extractor = DeepFeatureExtractor()\n",
    "print(\"Deep feature extractor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(dataset_path, output_path, sample_size=None):\n",
    "    \"\"\"Extract features from entire dataset\"\"\"\n",
    "    \n",
    "    # Collect all image paths and labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    image_paths.append(os.path.join(class_path, img_file))\n",
    "                    labels.append(class_name)\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and len(image_paths) > sample_size:\n",
    "        indices = np.random.choice(len(image_paths), sample_size, replace=False)\n",
    "        image_paths = [image_paths[i] for i in indices]\n",
    "        labels = [labels[i] for i in indices]\n",
    "    \n",
    "    print(f\"Extracting features from {len(image_paths)} images...\")\n",
    "    \n",
    "    # Initialize feature storage\n",
    "    traditional_features = []\n",
    "    deep_features = {'vgg16': [], 'resnet50': [], 'efficientnet': []}\n",
    "    valid_labels = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    # Extract features\n",
    "    for i, img_path in enumerate(tqdm(image_paths, desc=\"Extracting features\")):\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert to RGB\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize to standard size\n",
    "            img_resized = cv2.resize(img_rgb, (224, 224))\n",
    "            \n",
    "            # Extract traditional features\n",
    "            trad_feat = traditional_extractor.extract_all_features(img_resized)\n",
    "            traditional_features.append(trad_feat)\n",
    "            \n",
    "            # Extract deep features\n",
    "            deep_feat = deep_extractor.extract_all_model_features(img_resized)\n",
    "            for model_name, features in deep_feat.items():\n",
    "                deep_features[model_name].append(features)\n",
    "            \n",
    "            valid_labels.append(labels[i])\n",
    "            valid_paths.append(img_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    traditional_features = np.array(traditional_features)\n",
    "    for model_name in deep_features.keys():\n",
    "        deep_features[model_name] = np.array(deep_features[model_name])\n",
    "    \n",
    "    # Save features\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save traditional features\n",
    "    with open(output_path / 'traditional_features.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'features': traditional_features,\n",
    "            'labels': valid_labels,\n",
    "            'paths': valid_paths\n",
    "        }, f)\n",
    "    \n",
    "    # Save deep features\n",
    "    for model_name, features in deep_features.items():\n",
    "        with open(output_path / f'{model_name}_features.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'features': features,\n",
    "                'labels': valid_labels,\n",
    "                'paths': valid_paths\n",
    "            }, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'num_images': len(valid_labels),\n",
    "        'num_classes': len(set(valid_labels)),\n",
    "        'classes': list(set(valid_labels)),\n",
    "        'traditional_feature_dim': traditional_features.shape[1],\n",
    "        'deep_feature_dims': {name: feat.shape[1] for name, feat in deep_features.items()}\n",
    "    }\n",
    "    \n",
    "    with open(output_path / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFeature extraction complete!\")\n",
    "    print(f\"Processed {len(valid_labels)} images\")\n",
    "    print(f\"Traditional features shape: {traditional_features.shape}\")\n",
    "    for model_name, features in deep_features.items():\n",
    "        print(f\"{model_name} features shape: {features.shape}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Extract features from processed dataset\n",
    "processed_train_path = PROCESSED_DATA_PATH / 'plantvillage_color' / 'train'\n",
    "if processed_train_path.exists():\n",
    "    print(\"Extracting features from training set...\")\n",
    "    train_metadata = extract_features_from_dataset(\n",
    "        processed_train_path,\n",
    "        FEATURES_PATH / 'train',\n",
    "        sample_size=1000  # Sample for demonstration\n",
    "    )\n",
    "else:\n",
    "    print(\"Processed training data not found. Please run the preprocessing notebook first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
